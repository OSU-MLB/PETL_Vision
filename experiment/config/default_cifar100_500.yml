experiment_name: cifar100_tune_lrwd
pretrained_weights: vit_base_patch16_224_in21k
model: vit
optimizer: adamw
epoch: 100
warmup_epoch: 10
lr_min: 1.0e-5
warmup_lr_init: 1.0e-6
batch_size: 64
test_batch_size: 1024
crop_size: 224
final_run: False
ft_attn_module: null
ft_attn_mode: parallel
ft_attn_ln: before
ft_mlp_module: null
ft_mlp_mode: parallel
ft_mlp_ln: before
adapter_bottleneck: 64
adapter_init: lora_kaiming
adapter_scaler: 0.1
convpass_bottleneck: 8
convpass_xavier_init: False
convpass_init: lora_xavier
convpass_scaler: 10
vpt_mode: null
vpt_num: 10
vpt_layer: null
vpt_dropout: 0.1
ssf: False
lora_bottleneck: 0
fact_dim: 8
fact_type: null
fact_scaler: 1.0
repadapter_bottleneck: 8
repadapter_init: lora_xavier
repadapter_scaler: 1
repadapter_group: 2
bitfit: False
vqt_num: 0
vqt_dropout: 0.1
mlp_index: null
mlp_type: full
attention_index: null
attention_type: full
ln: False
difffit: False
full: False
block_index: null
gpu_num: 1
debug: False
random_seed: 42
eval_freq: 100
data_path: data_folder/cifar100
normalized: True
early_patience: 100
store_ckp: False
final_acc_hp: True
merge_factor: 1
final_acc_hp: True